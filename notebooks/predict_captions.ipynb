{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python366jvsc74a57bd0c91e85a08f47c59445e5414cbd0b26d902a00c31f50742f91abeee643b98c189",
   "display_name": "Python 3.6.6 64-bit ('venv': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from get_loader import (VideoDataset_to_VideoCaptionsLoader, Vocabulary,\n",
    "                        get_loader)\n",
    "from trainer import Trainer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = \"MSVD\"\n",
    "checkpoints_dir = os.path.join(\"checkpoints\", dataset)\n",
    "output_csv_dir = os.path.join(\"results\", dataset)\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "Initializing loader:\n",
      "Dataset: MSVD\n",
      "Split: val\n",
      "Video_only ?: False\n",
      "--------------------------------------------------\n",
      "Before integrity check: 3720\n",
      "After integrity check: 2562\n",
      "After removing unverified: 1055\n",
      "Loading Vocab: datasets\\MSVD\\metadata\\vocab.pkl \n",
      "--------------------------------------------------\n",
      "Initializing loader:\n",
      "Dataset: MSVD\n",
      "Split: val\n",
      "Video_only ?: False\n",
      "--------------------------------------------------\n",
      "Before integrity check: 3720\n",
      "After integrity check: 2562\n",
      "After removing unverified: 1055\n",
      "Loading Vocab: datasets\\MSVD\\metadata\\vocab.pkl \n"
     ]
    }
   ],
   "source": [
    "# Get the dataset and dataloaders\n",
    "\n",
    "dataset_folder = os.path.join(\"datasets\", dataset)\n",
    "vocab_pkl = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "vocab = Vocabulary.load(vocab_pkl)\n",
    "\n",
    "val_loader, _ = get_loader(\n",
    "    root_dir=dataset_folder,\n",
    "    dataset=dataset,\n",
    "    split=\"val\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_pkl=vocab_pkl,\n",
    ")\n",
    "test_loader, _ = get_loader(\n",
    "    root_dir=dataset_folder,\n",
    "    dataset=dataset,\n",
    "    split=\"val\",#\"test\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_pkl=vocab_pkl,\n",
    ")\n",
    "\n",
    "val_vidCap_loader = VideoDataset_to_VideoCaptionsLoader(val_loader.dataset, batch_size)\n",
    "test_vidCap_loader = VideoDataset_to_VideoCaptionsLoader(test_loader.dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "e of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is riding a man and his on a basketball .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is riding a bicycle on a stage .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is cutting a cat .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is playing a piano .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man plays a car .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is playing the guitar .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a man is a woman is slicing .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing the guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 568, 'reflen': 552, 'guess': [568, 498, 428, 358], 'correct': [386, 159, 82, 27]}\n",
      "ratio:1.028986\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.679577464787536, 'Bleu_2': 0.46580417335362845, 'Bleu_3': 0.3464114815602173, 'Bleu_4': 0.2366270518450272, 'ROUGE_L': 0.6466011902515647, 'CIDEr': 0.484800327661696}\n",
      "{'Bleu_1': 0.679577464787536, 'Bleu_2': 0.46580417335362845, 'Bleu_3': 0.3464114815602173, 'Bleu_4': 0.2366270518450272, 'ROUGE_L': 0.6466011902515647, 'CIDEr': 0.484800327661696}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a man is slicing bread .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a man is cutting a potato .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is riding a man .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is riding a man and his on a basketball .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is riding a bicycle on a stage .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is cutting a cat .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is playing a piano .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man plays a car .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is playing the guitar .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a man is a woman is slicing .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing the guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 568, 'reflen': 552, 'guess': [568, 498, 428, 358], 'correct': [386, 159, 82, 27]}\n",
      "ratio:1.028986\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.679577464787536, 'Bleu_2': 0.46580417335362845, 'Bleu_3': 0.3464114815602173, 'Bleu_4': 0.2366270518450272, 'ROUGE_L': 0.6466011902515647, 'CIDEr': 0.484800327661696}\n",
      "{'Bleu_1': 0.679577464787536, 'Bleu_2': 0.46580417335362845, 'Bleu_3': 0.3464114815602173, 'Bleu_4': 0.2366270518450272, 'ROUGE_L': 0.6466011902515647, 'CIDEr': 0.484800327661696}\n",
      "\n",
      "Loading model from checkpoint: SA-LSTM_30_epochs_video_local_0.5_5e-5_best.pt\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is cutting a piece of meat .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is putting some vegetables .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is riding a horse .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a basketball .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a guitar .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is playing with a <UNK> .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a dog is walking on a motorcycle .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is riding a horse on a horse .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is riding a horse in the field .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is slicing a <UNK> .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 606, 'reflen': 581, 'guess': [606, 536, 466, 396], 'correct': [396, 150, 80, 28]}\n",
      "ratio:1.043029\n",
      "{'Bleu_1': 0.6534653465335751, 'Bleu_2': 0.42763625300924407, 'Bleu_3': 0.3154648920361551, 'Bleu_4': 0.2170595941811215, 'ROUGE_L': 0.6243654188881725, 'CIDEr': 0.383495637931823}\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6534653465335751, 'Bleu_2': 0.42763625300924407, 'Bleu_3': 0.3154648920361551, 'Bleu_4': 0.2170595941811215, 'ROUGE_L': 0.6243654188881725, 'CIDEr': 0.383495637931823}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is cutting a piece of meat .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is putting some vegetables .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is riding a horse .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a basketball .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a guitar .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is playing with a <UNK> .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a dog is walking on a motorcycle .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is riding a horse on a horse .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is riding a horse in the field .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is slicing a <UNK> .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 606, 'reflen': 581, 'guess': [606, 536, 466, 396], 'correct': [396, 150, 80, 28]}\n",
      "ratio:1.043029\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6534653465335751, 'Bleu_2': 0.42763625300924407, 'Bleu_3': 0.3154648920361551, 'Bleu_4': 0.2170595941811215, 'ROUGE_L': 0.6243654188881725, 'CIDEr': 0.383495637931823}\n",
      "{'Bleu_1': 0.6534653465335751, 'Bleu_2': 0.42763625300924407, 'Bleu_3': 0.3154648920361551, 'Bleu_4': 0.2170595941811215, 'ROUGE_L': 0.6243654188881725, 'CIDEr': 0.383495637931823}\n",
      "\n",
      "Loading model from checkpoint: SA-LSTM_30_epochs_video_none_0.5_5e-5_best.pt\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a man is cutting some meat .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a man is slicing a potato .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is slicing a <UNK> .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a man .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is a car of a horse .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is riding a horse with a knife .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is riding a horse .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing a car .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is playing a guitar .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is slicing a <UNK> .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 532, 'reflen': 520, 'guess': [532, 462, 392, 322], 'correct': [372, 146, 78, 24]}\n",
      "ratio:1.023077\n",
      "{'Bleu_1': 0.6992481202994375, 'Bleu_2': 0.4700792637488951, 'Bleu_3': 0.35295302454417066, 'Bleu_4': 0.23926353965109912, 'ROUGE_L': 0.6550594240272442, 'CIDEr': 0.3617704945298932}\n",
      "{'Bleu_1': 0.6992481202994375, 'Bleu_2': 0.4700792637488951, 'Bleu_3': 0.35295302454417066, 'Bleu_4': 0.23926353965109912, 'ROUGE_L': 0.6550594240272442, 'CIDEr': 0.3617704945298932}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a man is cutting some meat .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a man is slicing a potato .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is slicing a <UNK> .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a man .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is a car of a horse .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is riding a horse with a knife .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is riding a horse .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing a car .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is playing a guitar .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is slicing a <UNK> .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 532, 'reflen': 520, 'guess': [532, 462, 392, 322], 'correct': [372, 146, 78, 24]}\n",
      "ratio:1.023077\n",
      "{'Bleu_1': 0.6992481202994375, 'Bleu_2': 0.4700792637488951, 'Bleu_3': 0.35295302454417066, 'Bleu_4': 0.23926353965109912, 'ROUGE_L': 0.6550594240272442, 'CIDEr': 0.3617704945298932}\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6992481202994375, 'Bleu_2': 0.4700792637488951, 'Bleu_3': 0.35295302454417066, 'Bleu_4': 0.23926353965109912, 'ROUGE_L': 0.6550594240272442, 'CIDEr': 0.3617704945298932}\n",
      "\n",
      "Loading model from checkpoint: SA-LSTM_30_epochs_video_only_best.pt\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is slicing a potato .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is adding something .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is slicing a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is cutting a car .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a piano .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a woman is riding a horse .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a woman is slicing a potato .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is holding a piece of bread .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is cutting a potato .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a person is cutting food .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 501, 'reflen': 504, 'guess': [501, 431, 361, 291], 'correct': [345, 121, 65, 21]}\n",
      "ratio:0.994048\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6845115861121003, 'Bleu_2': 0.43706312730819075, 'Bleu_3': 0.3245622277209201, 'Bleu_4': 0.22253837717400432, 'ROUGE_L': 0.6252465335355162, 'CIDEr': 0.34967413024591776}\n",
      "{'Bleu_1': 0.6845115861121003, 'Bleu_2': 0.43706312730819075, 'Bleu_3': 0.3245622277209201, 'Bleu_4': 0.22253837717400432, 'ROUGE_L': 0.6252465335355162, 'CIDEr': 0.34967413024591776}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is slicing a potato .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is adding something .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is slicing a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is cutting a car .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a piano .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a woman is riding a horse .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a woman is slicing a potato .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is holding a piece of bread .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is cutting a potato .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a person is cutting food .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 501, 'reflen': 504, 'guess': [501, 431, 361, 291], 'correct': [345, 121, 65, 21]}\n",
      "ratio:0.994048\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6845115861121003, 'Bleu_2': 0.43706312730819075, 'Bleu_3': 0.3245622277209201, 'Bleu_4': 0.22253837717400432, 'ROUGE_L': 0.6252465335355162, 'CIDEr': 0.34967413024591776}\n",
      "{'Bleu_1': 0.6845115861121003, 'Bleu_2': 0.43706312730819075, 'Bleu_3': 0.3245622277209201, 'Bleu_4': 0.22253837717400432, 'ROUGE_L': 0.6252465335355162, 'CIDEr': 0.34967413024591776}\n",
      "\n",
      "Loading model from checkpoint: SA-LSTM_30_epochs_video_only_global_0.5'_best.pt\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a man is pouring an egg .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a man is playing a <UNK> .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is cutting a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a <UNK> .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a guitar .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a woman is cutting a potato .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is slicing a potato .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing with a <UNK> .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is slicing a potato .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a man is playing with a potato .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a man .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 543, 'reflen': 535, 'guess': [543, 473, 403, 333], 'correct': [357, 124, 68, 20]}\n",
      "ratio:1.014953\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6574585635347008, 'Bleu_2': 0.41515900791927035, 'Bleu_3': 0.30752299749225215, 'Bleu_4': 0.20443471253855436, 'ROUGE_L': 0.6309925546593298, 'CIDEr': 0.2658502804647009}\n",
      "{'Bleu_1': 0.6574585635347008, 'Bleu_2': 0.41515900791927035, 'Bleu_3': 0.30752299749225215, 'Bleu_4': 0.20443471253855436, 'ROUGE_L': 0.6309925546593298, 'CIDEr': 0.2658502804647009}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a man is pouring an egg .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a man is playing a <UNK> .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is cutting a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a <UNK> .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is playing a guitar .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a woman is cutting a potato .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is slicing a potato .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing with a <UNK> .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a man is slicing a potato .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a man is playing with a potato .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a man .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 543, 'reflen': 535, 'guess': [543, 473, 403, 333], 'correct': [357, 124, 68, 20]}\n",
      "ratio:1.014953\n",
      "EVAL :   0%|          | 0/1 [00:00<?, ?it/s]{'Bleu_1': 0.6574585635347008, 'Bleu_2': 0.41515900791927035, 'Bleu_3': 0.30752299749225215, 'Bleu_4': 0.20443471253855436, 'ROUGE_L': 0.6309925546593298, 'CIDEr': 0.2658502804647009}\n",
      "{'Bleu_1': 0.6574585635347008, 'Bleu_2': 0.41515900791927035, 'Bleu_3': 0.30752299749225215, 'Bleu_4': 0.20443471253855436, 'ROUGE_L': 0.6309925546593298, 'CIDEr': 0.2658502804647009}\n",
      "\n",
      "Loading model from checkpoint: SA-LSTM_30_epochs_video_only_local_0.5'_best.pt\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is mixing eggs in a bowl .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is cutting a woman .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is cutting a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a potato .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is riding a horse .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is slicing a potato .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is playing a guitar .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing the guitar .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is cutting a piece of bread .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is cutting a potato .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 530, 'reflen': 527, 'guess': [530, 460, 390, 320], 'correct': [354, 128, 68, 18]}\n",
      "ratio:1.005693\n",
      "{'Bleu_1': 0.6679245283006265, 'Bleu_2': 0.4311116561268855, 'Bleu_3': 0.31881685893704614, 'Bleu_4': 0.20662677184450137, 'ROUGE_L': 0.6274115278723733, 'CIDEr': 0.24551454995655633}\n",
      "{'Bleu_1': 0.6679245283006265, 'Bleu_2': 0.4311116561268855, 'Bleu_3': 0.31881685893704614, 'Bleu_4': 0.20662677184450137, 'ROUGE_L': 0.6274115278723733, 'CIDEr': 0.24551454995655633}\n",
      "EVAL : 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "\n",
      "Example captions: key >> [generated] (ground_truth)\n",
      "bQJQGoJF7_k_162_169 >> [a woman is mixing eggs in a bowl .] (a man is filling up a bag with meat .)\n",
      "bSrpvMSuhPM_17_31 >> [a woman is cutting a woman .] (a man is talking to a woman .)\n",
      "bXsKw3TOQXs_30_55 >> [a woman is cutting a potato .] (a small piece of paper is being folded .)\n",
      "b_BuSVZwq6M_1_9 >> [a man is playing a potato .] (a man makes a great play in a cricket game .)\n",
      "bb6V0Grtub4_174_185 >> [a man is riding a horse .] (a man is playing on drums .)\n",
      "bkazguPsusc_74_85 >> [a man is slicing a potato .] (a cat is sliding under a couch .)\n",
      "bmxIurBrW5s_51_70 >> [a man is playing a guitar .] (a woman practicing a volleyball)\n",
      "bruzcOyIGeg_4_12 >> [a man is playing the guitar .] (a man drives a remote control car .)\n",
      "btuxO-C2IzE_64_72 >> [a woman is cutting a piece of bread .] (a lion jumps up and is hugged and petted by two long - <UNK> men .)\n",
      "buJ5HDCinrM_150_166 >> [a woman is cutting a potato .] (a woman is putting make up on her face .)\n",
      "bxDlC7YV5is_0_12 >> [a man is playing a guitar .] (a boy is playing a key - board between the people .)\n",
      "\n",
      "{'testlen': 530, 'reflen': 527, 'guess': [530, 460, 390, 320], 'correct': [354, 128, 68, 18]}\n",
      "ratio:1.005693\n",
      "{'Bleu_1': 0.6679245283006265, 'Bleu_2': 0.4311116561268855, 'Bleu_3': 0.31881685893704614, 'Bleu_4': 0.20662677184450137, 'ROUGE_L': 0.6274115278723733, 'CIDEr': 0.24551454995655633}\n",
      "{'Bleu_1': 0.6679245283006265, 'Bleu_2': 0.4311116561268855, 'Bleu_3': 0.31881685893704614, 'Bleu_4': 0.20662677184450137, 'ROUGE_L': 0.6274115278723733, 'CIDEr': 0.24551454995655633}\n"
     ]
    }
   ],
   "source": [
    "# Get a trainer for model evaluation\n",
    "tr = Trainer(checkpoint_name='test.ckpt', log_dir='trash')\n",
    "tr.device = device\n",
    "\n",
    "score_results = []\n",
    "\n",
    "# Load the models and predict\n",
    "for ckpt in os.listdir(checkpoints_dir):\n",
    "    if ckpt.endswith('_best.pt'):\n",
    "        print(\"\\nLoading model from checkpoint:\", ckpt)\n",
    "        model = torch.load(os.path.join(checkpoints_dir, ckpt))\n",
    "\n",
    "        for loader, phase in zip([val_vidCap_loader, test_vidCap_loader], ['val', 'test']):\n",
    "            # try:\n",
    "            loader.video_only = ('video_only' in ckpt)\n",
    "            scores, true_captions, generated_captions = tr.eval(model, loader, training_phase=phase, epoch=0, get_scores=True)\n",
    "\n",
    "            model_name = ckpt.split('_best')[0]\n",
    "            model_results = {\n",
    "                'generated_captions': generated_captions,\n",
    "                'true_captions': true_captions,\n",
    "            }\n",
    "\n",
    "            if not os.path.isdir(os.path.join(output_csv_dir, phase)):\n",
    "                os.makedirs(os.path.join(output_csv_dir, phase))\n",
    "\n",
    "            df = pd.DataFrame(model_results)\n",
    "            df.to_csv(\n",
    "                os.path.join(output_csv_dir, phase, f\"{model_name}.csv\"),\n",
    "                header=True,\n",
    "                columns=[\"generated_captions\", \"true_captions\"],\n",
    "            )\n",
    "\n",
    "            print(scores)\n",
    "            scores.update({'run': f\"{model_name}.csv\", 'split': phase})\n",
    "            score_results.append(scores)\n",
    "\n",
    "            # except:\n",
    "            #     print(f\"Could not compute {phase} results for {ckpt}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Bleu_1    Bleu_2    Bleu_3    Bleu_4   ROUGE_L     CIDEr  \\\n",
       "0   0.684601  0.461646  0.351056  0.246333  0.651561  0.384021   \n",
       "1   0.684601  0.461646  0.351056  0.246333  0.651561  0.384021   \n",
       "2   0.704673  0.481519  0.371008  0.268346  0.660898  0.406292   \n",
       "3   0.704673  0.481519  0.371008  0.268346  0.660898  0.406292   \n",
       "4   0.681905  0.454774  0.338185  0.227978  0.646632  0.407902   \n",
       "5   0.681905  0.454774  0.338185  0.227978  0.646632  0.407902   \n",
       "6   0.679577  0.465804  0.346411  0.236627  0.646601  0.484800   \n",
       "7   0.679577  0.465804  0.346411  0.236627  0.646601  0.484800   \n",
       "8   0.653465  0.427636  0.315465  0.217060  0.624365  0.383496   \n",
       "9   0.653465  0.427636  0.315465  0.217060  0.624365  0.383496   \n",
       "10  0.699248  0.470079  0.352953  0.239264  0.655059  0.361770   \n",
       "11  0.699248  0.470079  0.352953  0.239264  0.655059  0.361770   \n",
       "12  0.684512  0.437063  0.324562  0.222538  0.625247  0.349674   \n",
       "13  0.684512  0.437063  0.324562  0.222538  0.625247  0.349674   \n",
       "14  0.657459  0.415159  0.307523  0.204435  0.630993  0.265850   \n",
       "15  0.657459  0.415159  0.307523  0.204435  0.630993  0.265850   \n",
       "\n",
       "                                                  run split  \n",
       "0   SA-LSTM_30_epochs_video_audio_global_0.5_5e-5.csv   val  \n",
       "1   SA-LSTM_30_epochs_video_audio_global_0.5_5e-5.csv  test  \n",
       "2    SA-LSTM_30_epochs_video_audio_local_0.5_5e-5.csv   val  \n",
       "3    SA-LSTM_30_epochs_video_audio_local_0.5_5e-5.csv  test  \n",
       "4     SA-LSTM_30_epochs_video_audio_none_0.5_5e-5.csv   val  \n",
       "5     SA-LSTM_30_epochs_video_audio_none_0.5_5e-5.csv  test  \n",
       "6         SA-LSTM_30_epochs_video_global_0.5_5e-5.csv   val  \n",
       "7         SA-LSTM_30_epochs_video_global_0.5_5e-5.csv  test  \n",
       "8          SA-LSTM_30_epochs_video_local_0.5_5e-5.csv   val  \n",
       "9          SA-LSTM_30_epochs_video_local_0.5_5e-5.csv  test  \n",
       "10          SA-LSTM_30_epochs_video_none_0.5_5e-5.csv   val  \n",
       "11          SA-LSTM_30_epochs_video_none_0.5_5e-5.csv  test  \n",
       "12                   SA-LSTM_30_epochs_video_only.csv   val  \n",
       "13                   SA-LSTM_30_epochs_video_only.csv  test  \n",
       "14       SA-LSTM_30_epochs_video_only_global_0.5'.csv   val  \n",
       "15       SA-LSTM_30_epochs_video_only_global_0.5'.csv  test  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bleu_1</th>\n      <th>Bleu_2</th>\n      <th>Bleu_3</th>\n      <th>Bleu_4</th>\n      <th>ROUGE_L</th>\n      <th>CIDEr</th>\n      <th>run</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.684601</td>\n      <td>0.461646</td>\n      <td>0.351056</td>\n      <td>0.246333</td>\n      <td>0.651561</td>\n      <td>0.384021</td>\n      <td>SA-LSTM_30_epochs_video_audio_global_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.684601</td>\n      <td>0.461646</td>\n      <td>0.351056</td>\n      <td>0.246333</td>\n      <td>0.651561</td>\n      <td>0.384021</td>\n      <td>SA-LSTM_30_epochs_video_audio_global_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.704673</td>\n      <td>0.481519</td>\n      <td>0.371008</td>\n      <td>0.268346</td>\n      <td>0.660898</td>\n      <td>0.406292</td>\n      <td>SA-LSTM_30_epochs_video_audio_local_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.704673</td>\n      <td>0.481519</td>\n      <td>0.371008</td>\n      <td>0.268346</td>\n      <td>0.660898</td>\n      <td>0.406292</td>\n      <td>SA-LSTM_30_epochs_video_audio_local_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.681905</td>\n      <td>0.454774</td>\n      <td>0.338185</td>\n      <td>0.227978</td>\n      <td>0.646632</td>\n      <td>0.407902</td>\n      <td>SA-LSTM_30_epochs_video_audio_none_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.681905</td>\n      <td>0.454774</td>\n      <td>0.338185</td>\n      <td>0.227978</td>\n      <td>0.646632</td>\n      <td>0.407902</td>\n      <td>SA-LSTM_30_epochs_video_audio_none_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.679577</td>\n      <td>0.465804</td>\n      <td>0.346411</td>\n      <td>0.236627</td>\n      <td>0.646601</td>\n      <td>0.484800</td>\n      <td>SA-LSTM_30_epochs_video_global_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.679577</td>\n      <td>0.465804</td>\n      <td>0.346411</td>\n      <td>0.236627</td>\n      <td>0.646601</td>\n      <td>0.484800</td>\n      <td>SA-LSTM_30_epochs_video_global_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.653465</td>\n      <td>0.427636</td>\n      <td>0.315465</td>\n      <td>0.217060</td>\n      <td>0.624365</td>\n      <td>0.383496</td>\n      <td>SA-LSTM_30_epochs_video_local_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.653465</td>\n      <td>0.427636</td>\n      <td>0.315465</td>\n      <td>0.217060</td>\n      <td>0.624365</td>\n      <td>0.383496</td>\n      <td>SA-LSTM_30_epochs_video_local_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.699248</td>\n      <td>0.470079</td>\n      <td>0.352953</td>\n      <td>0.239264</td>\n      <td>0.655059</td>\n      <td>0.361770</td>\n      <td>SA-LSTM_30_epochs_video_none_0.5_5e-5.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.699248</td>\n      <td>0.470079</td>\n      <td>0.352953</td>\n      <td>0.239264</td>\n      <td>0.655059</td>\n      <td>0.361770</td>\n      <td>SA-LSTM_30_epochs_video_none_0.5_5e-5.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.684512</td>\n      <td>0.437063</td>\n      <td>0.324562</td>\n      <td>0.222538</td>\n      <td>0.625247</td>\n      <td>0.349674</td>\n      <td>SA-LSTM_30_epochs_video_only.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.684512</td>\n      <td>0.437063</td>\n      <td>0.324562</td>\n      <td>0.222538</td>\n      <td>0.625247</td>\n      <td>0.349674</td>\n      <td>SA-LSTM_30_epochs_video_only.csv</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.657459</td>\n      <td>0.415159</td>\n      <td>0.307523</td>\n      <td>0.204435</td>\n      <td>0.630993</td>\n      <td>0.265850</td>\n      <td>SA-LSTM_30_epochs_video_only_global_0.5'.csv</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.657459</td>\n      <td>0.415159</td>\n      <td>0.307523</td>\n      <td>0.204435</td>\n      <td>0.630993</td>\n      <td>0.265850</td>\n      <td>SA-LSTM_30_epochs_video_only_global_0.5'.csv</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# print(score_results)\n",
    "df = pd.DataFrame(score_results)\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MSR-VTT\"\n",
    "results_csv = os.path.join(\"results\", dataset, f\"NLP_score_{dataset}.csv\")\n",
    "df = pd.read_csv(results_csv)\n",
    "\n",
    "df = df[df['split'] == 'test']\n",
    "\n",
    "def get_reconstructor(x):\n",
    "    if 'global' in x: \n",
    "        return get_modalities(x) + ', Global'\n",
    "    if 'local' in x: \n",
    "        return get_modalities(x) + ', Local'\n",
    "    return '-'\n",
    "\n",
    "def get_modalities(x):\n",
    "    if 'video' in x and 'audio' in x: return 'A+V'\n",
    "    if 'video' in x: return 'V'\n",
    "    return '-'\n",
    "\n",
    "df['Reconstructor'] = df['run'].apply(lambda x: get_reconstructor(x))\n",
    "df['Modalities'] = df['run'].apply(lambda x: 'V' if 'video_only' in x else 'A+V')\n",
    "df['Decoder'] = 'SA-LSTM'\n",
    "\n",
    "df = df.groupby(['Decoder', 'Modalities', 'Reconstructor']).max().reset_index()\n",
    "\n",
    "df.head(15)\n",
    "\n",
    "df.round(3).to_latex(\n",
    "    results_csv.replace('.csv', '.tex'), \n",
    "    # columns=['Decoder', 'Modalities', 'Reconstructor', 'Bleu_4', 'ROUGE_L', 'CIDEr'],\n",
    "    columns=['Decoder', 'Modalities', 'Reconstructor', 'Bleu_4', 'METEOR', 'ROUGE_L', 'CIDEr'],\n",
    "\n",
    "    #['Decoder', 'Modalities', 'Reconstructor', 'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'METEOR', 'ROUGE_L', 'CIDEr'],\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}