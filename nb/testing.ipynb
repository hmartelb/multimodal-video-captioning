{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:07:14.139183Z",
     "start_time": "2021-06-17T02:07:14.101627Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:07:14.485101Z",
     "start_time": "2021-06-17T02:07:14.453334Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:07:16.594836Z",
     "start_time": "2021-06-17T02:07:14.850178Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:07:27.622950Z",
     "start_time": "2021-06-17T02:07:16.702834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ckh/anaconda3/lib/python3.6/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from get_loader import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T11:44:02.970980Z",
     "start_time": "2021-06-14T11:43:59.836968Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "train_loader, train_dataset = get_loader(root_dir=dataset_folder, split=\"train\", batch_size=1)\n",
    "val_loader, val_dataset = get_loader(root_dir=dataset_folder, split=\"val\", batch_size=1)\n",
    "test_loader, test_dataset = get_loader(root_dir=dataset_folder, split=\"test\", batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T11:44:17.377459Z",
     "start_time": "2021-06-14T11:44:17.008290Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 17, 1128]) torch.Size([12, 4])\n",
      "1 torch.Size([4, 44, 1128]) torch.Size([12, 4])\n",
      "2 torch.Size([4, 21, 1128]) torch.Size([9, 4])\n",
      "3 torch.Size([4, 17, 1128]) torch.Size([24, 4])\n",
      "4 torch.Size([4, 13, 1128]) torch.Size([7, 4])\n",
      "5 torch.Size([4, 44, 1128]) torch.Size([11, 4])\n",
      "6 torch.Size([4, 11, 1128]) torch.Size([9, 4])\n",
      "7 torch.Size([4, 17, 1128]) torch.Size([13, 4])\n",
      "8 torch.Size([4, 44, 1128]) torch.Size([9, 4])\n",
      "9 torch.Size([4, 21, 1128]) torch.Size([9, 4])\n",
      "10 torch.Size([4, 24, 1128]) torch.Size([13, 4])\n",
      "11 torch.Size([4, 11, 1128]) torch.Size([18, 4])\n",
      "12 torch.Size([4, 17, 1128]) torch.Size([10, 4])\n",
      "13 torch.Size([4, 24, 1128]) torch.Size([15, 4])\n",
      "14 torch.Size([4, 24, 1128]) torch.Size([12, 4])\n",
      "15 torch.Size([4, 19, 1128]) torch.Size([10, 4])\n",
      "16 torch.Size([4, 12, 1128]) torch.Size([9, 4])\n",
      "17 torch.Size([4, 44, 1128]) torch.Size([10, 4])\n",
      "18 torch.Size([4, 19, 1128]) torch.Size([7, 4])\n",
      "19 torch.Size([4, 44, 1128]) torch.Size([11, 4])\n",
      "20 torch.Size([4, 11, 1128]) torch.Size([10, 4])\n",
      "21 torch.Size([4, 21, 1128]) torch.Size([8, 4])\n",
      "22 torch.Size([4, 12, 1128]) torch.Size([10, 4])\n",
      "23 torch.Size([4, 24, 1128]) torch.Size([13, 4])\n",
      "24 torch.Size([2, 17, 1128]) torch.Size([11, 2])\n"
     ]
    }
   ],
   "source": [
    "tiny_loader, tiny_dataset = get_loader(root_dir=dataset_folder, split=\"tiny\", batch_size=4)\n",
    "\n",
    "for loader in [tiny_loader]:\n",
    "    for idx, (features, captions) in enumerate(loader):\n",
    "        print(idx, features.shape, captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T11:47:55.351247Z",
     "start_time": "2021-06-14T11:47:54.358593Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(features.shape[0]):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    sns.heatmap(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T11:50:26.965238Z",
     "start_time": "2021-06-14T11:50:26.855458Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(48.1306))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_feat = features[:,:,:1000]\n",
    "img_feat.min(), img_feat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T11:50:27.970578Z",
     "start_time": "2021-06-14T11:50:27.875152Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(510.))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feat = features[:,:,1000:]\n",
    "audio_feat.min(), audio_feat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vocab set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## pre-build vocab\n",
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "train_captions_file = os.path.join(dataset_folder, \"metadata\", \"train.csv\")\n",
    "val_captions_file = os.path.join(dataset_folder, \"metadata\", \"val.csv\")\n",
    "train_captions = pd.read_csv(train_captions_file)[\"Description\"].tolist()\n",
    "val_captions = pd.read_csv(val_captions_file)[\"Description\"].tolist()\n",
    "vocab_path = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "\n",
    "Vocabulary.prebuild(train_captions + val_captions, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary.load(vocab_path)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training loop - DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:08:13.891402Z",
     "start_time": "2021-06-16T07:08:13.699889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from get_loader import Vocabulary, get_loader\n",
    "from losses import (EntropyLoss, GlobalReconstructionLoss,\n",
    "                    LocalReconstructionLoss, TotalReconstructionLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:40:59.905565Z",
     "start_time": "2021-06-16T08:40:59.754855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.features_captioning import FeaturesCaptioning\n",
    "from models.captioning import AVCaptioning\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:41:01.982557Z",
     "start_time": "2021-06-16T08:41:01.834448Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab: ../datasets/MSVD/metadata/vocab.pkl \n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "vocab_pkl = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "tiny_loader, tiny_dataset = get_loader(root_dir=dataset_folder, split=\"tiny\", vocab_pkl=vocab_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:41:03.207180Z",
     "start_time": "2021-06-16T08:41:03.094285Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3201\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.load(vocab_pkl)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:12:47.368834Z",
     "start_time": "2021-06-16T08:12:47.144770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "decoder_config = {    \n",
    "    'rnn_type'       : 'LSTM', # ['LSTM', 'GRU']\n",
    "    'rnn_num_layers' : 1,\n",
    "    'rnn_bidirectional': False,  # Bool\n",
    "    'rnn_hidden_size': 512,\n",
    "    'rnn_dropout'    : 0.5,    \n",
    "    \n",
    "    'in_feature_size': 1000+128,\n",
    "    'embedding_size' : 128,\n",
    "    'attn_size'      : 128,\n",
    "    'output_size'    : 3201, #Vocab Size\n",
    "\n",
    "    'rnn_teacher_forcing_ratio' : 1.0,\n",
    "    'max_caption_len' : 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:12:48.464669Z",
     "start_time": "2021-06-16T08:12:48.362853Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "constructor_config = {   \n",
    "    'type'           : 'local',  # ['global', 'local']\n",
    "    'rnn_type'       : 'LSTM',    # ['LSTM', 'GRU']\n",
    "    'rnn_num_layers' : 1,\n",
    "    'rnn_bidirectional': False,     # Bool\n",
    "    'hidden_size'    : 512,       # feature_size\n",
    "    'rnn_dropout'    : 0.5,    \n",
    "    'decoder_size'   : 128,       # decoder_hidden_size\n",
    "    'attn_size'      : 128,       # only applied for local\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:20:03.664022Z",
     "start_time": "2021-06-16T08:19:41.175378Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "TRAIN:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "\n",
      "Epoch 1/1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN: 100%|██████████| 4/4 [00:19<00:00,  4.82s/it, total=8.09, ce=8.09, e=2.77e+3, recon=1.2e+3]\n",
      "TEST : 100%|██████████| 4/4 [00:02<00:00,  1.41it/s, total=7.87, ce=7.87, e=2.58e+3, recon=1.89e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1000000.0 to 7.840059161186218.\n",
      "Saving checkpoint to: checkpoints/test.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [{'total': 8.017806887626648,\n",
       "   'ce': 8.017806887626648,\n",
       "   'e': 4101.0902099609375,\n",
       "   'recon': 1813.4128112792969}],\n",
       " 'val_loss': [{'total': 7.840059161186218,\n",
       "   'ce': 7.840059161186218,\n",
       "   'e': 4229.341003417969,\n",
       "   'recon': 1956.0495910644531}],\n",
       " 'test_loss': []}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.features_captioning import FeaturesCaptioning\n",
    "from models.reconstructor import GlobalReconstructor, LocalReconstructor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = decoder_config.copy()\n",
    "config['output_size'] = len(vocab)\n",
    "model = FeaturesCaptioning(**config)\n",
    "model.to(device)\n",
    "\n",
    "rec_config = constructor_config.copy()\n",
    "rec_config['decoder_size'] = config['rnn_hidden_size']\n",
    "rec_config['hidden_size'] = config['in_feature_size']\n",
    "# reconstructor = GlobalReconstructor(**rec_config,device=device)\n",
    "reconstructor = LocalReconstructor(**rec_config,device=device)\n",
    "reconstructor = reconstructor.to(device)\n",
    "\n",
    "print(\"Start training\")\n",
    "tr = Trainer(checkpoint_name=os.path.join(\"checkpoints\", \"test.ckpt\"))\n",
    "tr.fit(\n",
    "    model,\n",
    "    reconstructor,\n",
    "    tiny_loader,\n",
    "    tiny_loader,\n",
    "    device,\n",
    "    epochs=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:13:05.596286Z",
     "start_time": "2021-06-16T08:13:05.139914Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 44, 1128]), torch.Size([15, 32]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = iter(tiny_loader).next()\n",
    "x,y = data\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:13:07.685284Z",
     "start_time": "2021-06-16T08:13:06.859959Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 32, 3201]), torch.Size([15, 1, 32, 512]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, hidden = model.decode(x,y,max_caption_len=y.shape[0])\n",
    "outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:13:09.020009Z",
     "start_time": "2021-06-16T08:13:08.739248Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 1128])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_rec = reconstructor.reconstruct(hidden, outputs, y)\n",
    "feat_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:14:11.868046Z",
     "start_time": "2021-06-16T08:14:11.705623Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../losses.py:31: UserWarning: Using a target size (torch.Size([32, 15, 1128])) that is different to the input size (torch.Size([32, 44, 1128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(x, x_recon)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (44) must match the size of tensor b (15) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-200965c06191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLocalReconstructionLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/清华TsingHua/Sem2/NLP/Assignments/project/Video-Captioning-AV/losses.py\u001b[0m in \u001b[0;36mLocalReconstructionLoss\u001b[0;34m(x, x_recon)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLocalReconstructionLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2536\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (44) must match the size of tensor b (15) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "LocalReconstructionLoss(x, feat_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T08:13:10.042794Z",
     "start_time": "2021-06-16T08:13:09.935807Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 44, 1128]), torch.Size([32, 15, 1128]), torch.Size([15, 32]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, feat_rec.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:28:26.294757Z",
     "start_time": "2021-06-16T07:28:26.145368Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 18, 1128]), torch.Size([32, 1128]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_recon = feat_rec\n",
    "keep_mask = y != 0\n",
    "xx = x.mean(dim=1)\n",
    "\n",
    "x_recon.shape, xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:28:27.622920Z",
     "start_time": "2021-06-16T07:28:27.514632Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1128])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_len = keep_mask.sum(dim=0)\n",
    "caption_len = caption_len.unsqueeze(1).expand(caption_len.size(0), x_recon.size(2))\n",
    "\n",
    "caption_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:28:28.973812Z",
     "start_time": "2021-06-16T07:28:28.863611Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 32]), torch.Size([18, 32]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, keep_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:28:38.773106Z",
     "start_time": "2021-06-16T07:28:38.607232Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 18, 1128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_mask = y != 0\n",
    "keep_mask = keep_mask.transpose(0, 1).unsqueeze(2).expand_as(x_recon)\n",
    "keep_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T07:28:44.776151Z",
     "start_time": "2021-06-16T07:28:44.630223Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(229.8353, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_len = caption_len.type(torch.FloatTensor).to(x_recon.device)\n",
    "keep_mask = keep_mask.type(torch.FloatTensor).to(x_recon.device)\n",
    "\n",
    "x_recon = keep_mask * x_recon\n",
    "x_recon = x_recon.sum(dim=1) / caption_len\n",
    "\n",
    "F.mse_loss(xx, x_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T15:51:30.072910Z",
     "start_time": "2021-06-14T15:51:29.745643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## load config\n",
    "decoder_config = {\n",
    "    'rnn_num_layers' : 1,\n",
    "    'rnn_bidirection': False,\n",
    "    'rnn_hidden_size': 512,\n",
    "    'rnn_attn_size'  : 256,\n",
    "    'rnn_dropout'    : 0.5, \n",
    "    \n",
    "    #     rnn_type = 'LSTM'; assert rnn_type in [ 'LSTM', 'GRU' ]\n",
    "    #     rnn_teacher_forcing_ratio = 1.0\n",
    "}\n",
    "\n",
    "## load dataset\n",
    "dataloader = tiny_loader\n",
    "\n",
    "## load model\n",
    "## tensor board\n",
    "\n",
    "## optimizer\n",
    "## lr_scheduler\n",
    "\n",
    "## loop\n",
    "## train \n",
    "## val - keep best\n",
    "\n",
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T09:50:32.157291Z",
     "start_time": "2021-06-16T09:50:32.038377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.captioning import AVCaptioning\n",
    "from trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T09:50:32.759687Z",
     "start_time": "2021-06-16T09:50:32.636749Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab: ../datasets/MSVD/metadata/vocab.pkl \n",
      "3201\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "vocab_pkl = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "tiny_loader, tiny_dataset = get_loader(root_dir=dataset_folder, split=\"tiny\", vocab_pkl=vocab_pkl)\n",
    "\n",
    "vocab = Vocabulary.load(vocab_pkl)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T09:50:57.256998Z",
     "start_time": "2021-06-16T09:50:33.191582Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "TRAIN:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n",
      "Decoder      : LSTM In 1128 Out 3201 Hidden 512\n",
      "Reconstuctor : global\n",
      "Start training\n",
      "\n",
      "Epoch 1/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN: 100%|██████████| 4/4 [00:11<00:00,  2.82s/it, total=8.19, ce=8.19, e=2.97e+3, recon=277]\n",
      "TEST : 100%|██████████| 4/4 [00:03<00:00,  1.32it/s, total=7.93, ce=7.93, e=2.78e+3, recon=398]\n",
      "TRAIN:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1000000.0 to 7.959479212760925.\n",
      "Saving checkpoint to: checkpoints/test.ckpt\n",
      "\n",
      "Epoch 2/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN:  50%|█████     | 2/4 [00:09<00:09,  4.61s/it, total=7.92, ce=7.92, e=2.58e+3, recon=375]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-12fbd348e1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtiny_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;32m~/Desktop/清华TsingHua/Sem2/NLP/Assignments/project/Video-Captioning-AV/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_loader, val_loader, device, train_config)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch}/{train_config.epochs}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/清华TsingHua/Sem2/NLP/Assignments/project/Video-Captioning-AV/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, dataloader)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mfeatures_recons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 )\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_clip_value\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AVCaptioning(\n",
    "    vocab_size=len(vocab),\n",
    "    teacher_forcing_ratio=0.0,\n",
    "    device=device,\n",
    ") \n",
    "model.to(device)\n",
    "\n",
    "train_config = TrainerConfig()\n",
    "\n",
    "print(\"Start training\")\n",
    "tr = Trainer(checkpoint_name=os.path.join(\"checkpoints\", \"test.ckpt\"))\n",
    "tr.fit(\n",
    "    model,\n",
    "    tiny_loader,\n",
    "    tiny_loader,\n",
    "    device,\n",
    "    train_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T09:46:53.549556Z",
     "start_time": "2021-06-16T09:46:53.311972Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a8f8aae7e794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_config' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bean Search - DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:38:18.381149Z",
     "start_time": "2021-06-17T06:38:18.046219Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.features_captioning import FeaturesCaptioning\n",
    "from models.captioning import AVCaptioning\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:38:18.917262Z",
     "start_time": "2021-06-17T06:38:18.791283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab: ../datasets/MSVD/metadata/vocab.pkl \n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "vocab_pkl = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "tiny_loader, tiny_dataset = get_loader(root_dir=dataset_folder, split=\"tiny\", vocab_pkl=vocab_pkl, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:06:56.932974Z",
     "start_time": "2021-06-17T06:06:56.836664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3201\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.load(vocab_pkl)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:07:02.454016Z",
     "start_time": "2021-06-17T06:07:02.336160Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_config = {    \n",
    "    'rnn_type'       : 'LSTM', # ['LSTM', 'GRU']\n",
    "    'rnn_num_layers' : 1,\n",
    "    'rnn_bidirectional': False,  # Bool\n",
    "    'rnn_hidden_size': 512,\n",
    "    'rnn_dropout'    : 0.5,    \n",
    "    \n",
    "    'in_feature_size': 1000+128,\n",
    "    'embedding_size' : 128,\n",
    "    'attn_size'      : 128,\n",
    "    'output_size'    : 3201, #Vocab Size\n",
    "\n",
    "    'rnn_teacher_forcing_ratio' : 1.0,\n",
    "    'max_caption_len' : 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:07:04.875824Z",
     "start_time": "2021-06-17T06:07:04.706369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesCaptioning(\n",
       "  (embedding): Embedding(3201, 128)\n",
       "  (attention): TemporalAttention(\n",
       "    (W): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (U): Linear(in_features=1128, out_features=128, bias=False)\n",
       "    (w): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       "  (rnn): LSTM(1256, 512)\n",
       "  (out): Linear(in_features=512, out_features=3201, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = decoder_config.copy()\n",
    "config['output_size'] = len(vocab)\n",
    "model = FeaturesCaptioning(**config)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:07:07.289603Z",
     "start_time": "2021-06-17T06:07:07.186409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 1128]), torch.Size([10, 2]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = iter(tiny_loader).next()\n",
    "x,y = data\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:07:09.744404Z",
     "start_time": "2021-06-17T06:07:09.492973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 2, 3201]), torch.Size([30, 1, 2, 512]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, hidden = model.decode(x,max_caption_len=30)\n",
    "outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:07:18.520664Z",
     "start_time": "2021-06-17T06:07:18.431092Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_rnn_init_hidden(batch_size, num_layers, num_directions, hidden_size):\n",
    "    if model.rnn_type == 'LSTM':\n",
    "        hidden = (\n",
    "            torch.zeros(num_layers * num_directions, batch_size, hidden_size),\n",
    "            torch.zeros(num_layers * num_directions, batch_size, hidden_size))\n",
    "    else:\n",
    "        hidden = torch.zeros(num_layers * num_directions, batch_size, hidden_size)\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:09:00.374000Z",
     "start_time": "2021-06-17T06:09:00.268950Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_caption_len = 30\n",
    "feats = x\n",
    "alpha = 0\n",
    "width = 5\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "hidden = get_rnn_init_hidden(c, model.num_layers, model.num_directions,\n",
    "                                          model.hidden_size)\n",
    "# [2 LSTM ] * [1 * batch * 512]\n",
    "\n",
    "input_list = [ torch.LongTensor(1, batch_size).fill_(vocab.stoi['<SOS>']) ]\n",
    "# [<SOS> * batch]\n",
    "\n",
    "hidden_list = [ hidden ]\n",
    "\n",
    "cum_prob_list = [ torch.ones(batch_size) ]\n",
    "cum_prob_list = [ torch.log(cum_prob) for cum_prob in cum_prob_list ]\n",
    "EOS_idx = vocab.stoi['<EOS>']\n",
    "\n",
    "output_list = [ [[]] for _ in range(batch_size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:11:30.655224Z",
     "start_time": "2021-06-17T06:11:30.561988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1., 1.])]\n",
      "[tensor([0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "cum_prob_list = [ torch.ones(batch_size) ]\n",
    "print(cum_prob_list)\n",
    "cum_prob_list = [ torch.log(cum_prob) for cum_prob in cum_prob_list ]\n",
    "print(cum_prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:18:13.033292Z",
     "start_time": "2021-06-17T06:18:11.548077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178),\n",
       "  tensor(1323),\n",
       "  tensor(1178)],\n",
       " [1,\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582),\n",
       "  tensor(2582)]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(max_caption_len + 1):\n",
    "    beam_output_list = [] # width x ( 1, 100 )\n",
    "    normalized_beam_output_list = [] # width x ( 1, 100 )\n",
    "    if model.rnn_type == \"LSTM\":\n",
    "        beam_hidden_list = ( [], [] ) # 2 * width x ( 1, 100, 512 )\n",
    "    else:\n",
    "        beam_hidden_list = [] # width x ( 1, 100, 512 )\n",
    "    next_output_list = [ [] for _ in range(batch_size) ]\n",
    "\n",
    "    assert len(input_list) == len(hidden_list) == len(cum_prob_list)\n",
    "    for i, (input, hidden, cum_prob) in enumerate(zip(input_list, hidden_list, cum_prob_list)):\n",
    "        output, next_hidden, _ = model.forward_word(feats, hidden, input)\n",
    "\n",
    "        caption_list = [ output_list[b][i] for b in range(batch_size)]\n",
    "        EOS_mask = [ 0. if EOS_idx in [ idx.item() for idx in caption ] else 1. for caption in caption_list ]\n",
    "        EOS_mask = torch.FloatTensor(EOS_mask)\n",
    "        EOS_mask = EOS_mask.unsqueeze(1).expand_as(output)\n",
    "        output = EOS_mask * output\n",
    "\n",
    "        output += cum_prob.unsqueeze(1)\n",
    "        beam_output_list.append(output)\n",
    "\n",
    "        caption_lens = [ [ idx.item() for idx in caption ].index(EOS_idx) + 1 if EOS_idx in [ idx.item() for idx in caption ] else t + 1 for caption in caption_list ]\n",
    "        caption_lens = torch.FloatTensor(caption_lens)\n",
    "        normalizing_factor = ((5 + caption_lens) ** alpha) / (6 ** alpha)\n",
    "        normalizing_factor = normalizing_factor.unsqueeze(1).expand_as(output)\n",
    "        normalized_output = output / normalizing_factor\n",
    "        normalized_beam_output_list.append(normalized_output)\n",
    "        if model.rnn_type == \"LSTM\":\n",
    "            beam_hidden_list[0].append(next_hidden[0])\n",
    "            beam_hidden_list[1].append(next_hidden[1])\n",
    "        else:\n",
    "            beam_hidden_list.append(next_hidden)\n",
    "    beam_output_list = torch.cat(beam_output_list, dim=1) # ( 100, n_vocabs * width )\n",
    "    normalized_beam_output_list = torch.cat(normalized_beam_output_list, dim=1)\n",
    "    beam_topk_output_index_list = normalized_beam_output_list.argsort(dim=1, descending=True)[:, :width] # ( 100, width )\n",
    "    topk_beam_index = beam_topk_output_index_list // vocab_size # ( 100, width )\n",
    "    topk_output_index = beam_topk_output_index_list % vocab_size # ( 100, width )\n",
    "\n",
    "    topk_output_list = [ topk_output_index[:, i] for i in range(width) ] # width * ( 100, )\n",
    "    if model.rnn_type == \"LSTM\":\n",
    "        topk_hidden_list = (\n",
    "            [ [] for _ in range(width) ],\n",
    "            [ [] for _ in range(width) ]) # 2 * width * (1, 100, 512)\n",
    "    else:\n",
    "        topk_hidden_list = [ [] for _ in range(width) ] # width * ( 1, 100, 512 )\n",
    "    topk_cum_prob_list = [ [] for _ in range(width) ] # width * ( 100, )\n",
    "    for i, (beam_index, output_index) in enumerate(zip(topk_beam_index, topk_output_index)):\n",
    "        for k, (bi, oi) in enumerate(zip(beam_index, output_index)):\n",
    "            if model.rnn_type == \"LSTM\":\n",
    "                topk_hidden_list[0][k].append(beam_hidden_list[0][bi][:, i, :])\n",
    "                topk_hidden_list[1][k].append(beam_hidden_list[1][bi][:, i, :])\n",
    "            else:\n",
    "                topk_hidden_list[k].append(beam_hidden_list[bi][:, i, :])\n",
    "            topk_cum_prob_list[k].append(beam_output_list[i][vocab_size * bi + oi])\n",
    "            next_output_list[i].append(output_list[i][bi] + [ oi ])\n",
    "    output_list = next_output_list\n",
    "\n",
    "    input_list = [ topk_output.unsqueeze(0) for topk_output in topk_output_list ] # width * ( 1, 100 )\n",
    "    if model.rnn_type == \"LSTM\":\n",
    "        hidden_list = (\n",
    "            [ torch.stack(topk_hidden, dim=1) for topk_hidden in topk_hidden_list[0] ],\n",
    "            [ torch.stack(topk_hidden, dim=1) for topk_hidden in topk_hidden_list[1] ]) # 2 * width * ( 1, 100, 512 )\n",
    "        hidden_list = [ ( hidden, context ) for hidden, context in zip(*hidden_list) ]\n",
    "    else:\n",
    "        hidden_list = [ torch.stack(topk_hidden, dim=1) for topk_hidden in topk_hidden_list ] # width * ( 1, 100, 512 )\n",
    "    cum_prob_list = [ torch.FloatTensor(topk_cum_prob) for topk_cum_prob in topk_cum_prob_list ] # width * ( 100, )\n",
    "\n",
    "SOS_idx = vocab.stoi['<SOS>']\n",
    "outputs = [ [ SOS_idx ] + o[0] for o in output_list ]\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:31:08.712222Z",
     "start_time": "2021-06-17T02:31:08.599632Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:39:42.437655Z",
     "start_time": "2021-06-17T02:39:42.331938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor(outputs)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:37:28.077203Z",
     "start_time": "2021-06-17T02:37:27.980185Z"
    }
   },
   "outputs": [],
   "source": [
    "EOS_idx = vocab.stoi['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:37:38.277175Z",
     "start_time": "2021-06-17T02:37:38.182184Z"
    }
   },
   "outputs": [],
   "source": [
    "p = pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:37:45.065443Z",
     "start_time": "2021-06-17T02:37:44.937092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1, 2148, 2148, 2148, 2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906,\n",
       "        2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906,\n",
       "        2906, 2906, 2906, 2906, 2906, 2906, 2906, 2906])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T02:40:24.672467Z",
     "start_time": "2021-06-17T02:40:24.495453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleepy sleepy sleepy fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork almost brings brings brings brings brings brings brings brings herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself sleepy hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen : enjoying enjoying enjoying enjoying enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen uses uses uses lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick lipstick stairs almost brings brings brings brings brings brings brings brings herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself herself hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen : : tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray sleepy sleepy sleepy fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork sleepy hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches catches handgun practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced sleepy sleepy sleepy fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork fork papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers brings brings brings brings brings brings brings brings brings poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll poll wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife wife : : tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat sneezes bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat sneezes sneezes sneezes sneezes bat booth booth booth booth booth booth booth booth rocks booth rocks booth rocks thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly thinly papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers papers dropping dropping dropping dropping dropping watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched watched : enjoying enjoying enjoying enjoying enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth enjoying booth : : tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray tray hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen hen uses uses uses lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick stairs lipstick lipstick stairs handgun practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced practiced looking notebook brings notebook looking booth saw booth saw booth saw booth saw guttier guttier guttier guttier booth saw guttier guttier guttier guttier guttier guttier guttier guttier guttier guttier guttier guttier death death death death death uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses uses notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook notebook creating creating creating talk talk border border border border border border border border border border border border border border border border border border border border border border border border border border "
     ]
    }
   ],
   "source": [
    "for p in pred:\n",
    "    for idx in p[1:]:\n",
    "        idx = idx.item()\n",
    "        if idx == EOS_idx:\n",
    "            break\n",
    "        print(vocab.itos[idx], end = ' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:34.582993Z",
     "start_time": "2021-06-17T06:54:33.800745Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.features_captioning import FeaturesCaptioning\n",
    "from models.captioning import AVCaptioning\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:35.392318Z",
     "start_time": "2021-06-17T06:54:35.271851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab: ../datasets/MSVD/metadata/vocab.pkl \n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join(\"../datasets\", \"MSVD\")\n",
    "vocab_pkl = os.path.join(dataset_folder, \"metadata\", \"vocab.pkl\")\n",
    "tiny_loader, tiny_dataset = get_loader(root_dir=dataset_folder, split=\"tiny\", vocab_pkl=vocab_pkl, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:35.922585Z",
     "start_time": "2021-06-17T06:54:35.826054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3201\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.load(vocab_pkl)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:36.396387Z",
     "start_time": "2021-06-17T06:54:36.276101Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_config = {    \n",
    "    'rnn_type'       : 'LSTM', # ['LSTM', 'GRU']\n",
    "    'rnn_num_layers' : 1,\n",
    "    'rnn_bidirectional': False,  # Bool\n",
    "    'rnn_hidden_size': 512,\n",
    "    'rnn_dropout'    : 0.5,    \n",
    "    \n",
    "    'in_feature_size': 1000+128,\n",
    "    'embedding_size' : 128,\n",
    "    'attn_size'      : 128,\n",
    "    'output_size'    : 3201, #Vocab Size\n",
    "\n",
    "    'rnn_teacher_forcing_ratio' : 1.0,\n",
    "    'max_caption_len' : 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:36.940542Z",
     "start_time": "2021-06-17T06:54:36.776089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesCaptioning(\n",
       "  (embedding): Embedding(3201, 128)\n",
       "  (attention): TemporalAttention(\n",
       "    (W): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (U): Linear(in_features=1128, out_features=128, bias=False)\n",
       "    (w): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       "  (rnn): LSTM(1256, 512)\n",
       "  (out): Linear(in_features=512, out_features=3201, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = decoder_config.copy()\n",
    "config['output_size'] = len(vocab)\n",
    "model = FeaturesCaptioning(**config)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:37.370625Z",
     "start_time": "2021-06-17T06:54:37.259314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 11, 1128]), torch.Size([10, 2]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = iter(tiny_loader).next()\n",
    "x,y = data\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:37.909340Z",
     "start_time": "2021-06-17T06:54:37.648314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 2, 3201]), torch.Size([30, 1, 2, 512]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, hidden = model.decode(x,max_caption_len=30)\n",
    "outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:54:39.519333Z",
     "start_time": "2021-06-17T06:54:38.183376Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model.beam_search_predict(x, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
